from qdrant_client import QdrantClient
from qdrant_client.http import models
from typing import List, Dict, Any, Optional
import voyageai
import os
from dotenv import load_dotenv
load_dotenv()
import numpy as np
from typing import List
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class QdrantService:
    """
    QdrantService: tiá»‡n Ã­ch quáº£n lÃ½ collection, upsert, vÃ  search vector.
    DÃ¹ng cho RAG, knowledge retrieval, vÃ  AI embeddings.
    """

    def __init__(self):
        self.url = "http://34.87.152.106:6333"
        self.collection_name = "toeic_speaking_knowledge"
        self.dimension = 1024
        self.client = QdrantClient(url=self.url)
        self.vo = voyageai.Client(api_key=os.environ.get("VOYAGE_API_KEY"))


    def connect(self, auto_create: bool = True):
        """
        Káº¿t ná»‘i tá»›i Qdrant vÃ  VoyageAI, kiá»ƒm tra cáº¥u hÃ¬nh cÆ¡ báº£n.
        - auto_create=True: tá»± táº¡o collection náº¿u chÆ°a cÃ³.
        """
        print("ðŸ”— Checking Qdrant and VoyageAI connection...")

        try:
            health = self.client.get_collections()
            if health:
                print(f"âœ… Connected to Qdrant at {self.url}")
        except Exception as e:
            raise ConnectionError(f"âŒ Cannot connect to Qdrant at {self.url} â€” {e}")

        try:
            info = self.client.get_collection(self.collection_name)
            vector_size = info.config.params.vectors.size
            print(f"ðŸ“¦ Collection '{self.collection_name}' exists with {vector_size} dims.")
        except Exception:
            if auto_create:
                self.create_collection(vector_size=self.dimension)
            else:
                raise RuntimeError(
                    f"âš ï¸ Collection '{self.collection_name}' not found and auto_create=False."
                )

        # try:
        #     test_vec = self.vo.embed(
        #         model="voyage-3-large",
        #         texts=["connection test"],
        #         output_dimension=self.dimension
        #     )
        #     if test_vec and hasattr(test_vec, "embeddings"):
        #         print("âœ… VoyageAI API is active and returning embeddings.")
        # except Exception as e:
        #     raise ConnectionError(f"âŒ VoyageAI API check failed â€” {e}")

        print("ðŸš€ Connection and setup complete!")
        return True

    def get_embedding(self, text: str) -> List[float]:
        """Generate embedding using VoyageAI"""
        response = self.vo.embed(
            model="voyage-3-large", 
            texts=[text], 
            output_dimension=self.dimension
        )
        return response.embeddings[0]

    def create_collection(
        self,
        vector_size: int = 1024,
        distance: models.Distance = models.Distance.COSINE,
        recreate: bool = False
    ):
        """
        Táº¡o hoáº·c tÃ¡i táº¡o collection (náº¿u recreate=True).
        """
        if recreate:
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(size=vector_size, distance=distance)
            )
            print(f"â™»ï¸ Recreated collection '{self.collection_name}' with {vector_size} dims.")
        else:
            try:
                self.client.get_collection(self.collection_name)
                print(f"âœ… Collection '{self.collection_name}' already exists.")
            except Exception:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(size=vector_size, distance=distance)
                )
                print(f"âœ¨ Created new collection '{self.collection_name}' ({vector_size} dims).")

   
    def upsert_points(self, points: List[Dict[str, Any]]):
        """
        Upsert dá»¯ liá»‡u vector vÃ o collection.
        points pháº£i cÃ³ dáº¡ng:
        [
          { "id": 1, "vector": [...], "payload": {...} },
          ...
        ]
        """
        qdrant_points = [
            models.PointStruct(id=p["id"], vector=p["vector"], payload=p["payload"])
            for p in points
        ]
        if not qdrant_points:
            print("âš ï¸ No points to upsert â€” skipping.")
            return

        self.client.upsert(collection_name=self.collection_name, points=qdrant_points)
        print(f"âœ… Upserted {len(qdrant_points)} points into '{self.collection_name}'.")

    
    def search(
        self,
        question: str,
        limit: int = 3,
        score_threshold: Optional[float] = 0
    ) -> List[Dict[str, Any]]:
        """
        Truy váº¥n cÃ¡c vector gáº§n nháº¥t trong Qdrant.
        """
        query = refine_query_with_tfidf_mmr(question)
        query_vector = self.get_embedding(query)

        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit
        )

        formatted = []
        for r in results:
            if score_threshold and r.score < score_threshold:
                continue
            formatted.append({
                "id": r.id,
                "score": r.score,
                "payload": r.payload
            })

        print(f"ðŸ” Search returned {len(formatted)} results for question: '{question}'")
        return formatted

    
    def delete_collection(self):
        """XoÃ¡ collection hoÃ n toÃ n."""
        self.client.delete_collection(self.collection_name)
        print(f"ðŸ—‘ï¸ Deleted collection '{self.collection_name}'.")


def mmr_select(query_vec, candidate_vecs, Î»=0.7, top_k=5):
    """Chá»n top_k vector báº±ng Maximum Marginal Relevance"""
    sim_query = cosine_similarity([query_vec], candidate_vecs)[0]
    sim_cand = cosine_similarity(candidate_vecs)

    selected = []
    candidates = list(range(len(candidate_vecs)))

    while len(selected) < top_k and candidates:
        scores = [
            Î» * sim_query[i] - (1 - Î») * max([sim_cand[i][j] for j in selected] or [0])
            for i in candidates
        ]
        chosen = candidates[np.argmax(scores)]
        selected.append(chosen)
        candidates.remove(chosen)

    return selected

def refine_query_with_tfidf_mmr(query: str, corpus: list[str] = None, top_n=10, Î»=0.7) -> str:
    """
    LÃ m giÃ u query trÆ°á»›c khi embedding:
    - TF-IDF Ä‘á»ƒ chá»n cá»¥m quan trá»ng
    - MMR Ä‘á»ƒ giá»¯ cÃ¡c cá»¥m Ä‘a dáº¡ng nháº¥t
    """
    if corpus is None:
        corpus = [query]

    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
    tfidf = vectorizer.fit_transform(corpus + [query])
    feature_names = vectorizer.get_feature_names_out()
    query_vec = tfidf[-1].toarray()[0]

    top_indices = np.argsort(query_vec)[::-1][:top_n]
    top_terms = [feature_names[i] for i in top_indices if query_vec[i] > 0]

    if not top_terms:
        return query  # fallback

    term_vecs = vectorizer.transform(top_terms).toarray()
    query_center = np.mean(term_vecs, axis=0)
    selected_ids = mmr_select(query_center, term_vecs, Î»=Î», top_k=min(5, len(term_vecs)))

    selected_terms = [top_terms[i] for i in selected_ids]
    refined_query = query + ". " + ". ".join(selected_terms)
    return refined_query


# --------------------------------------------------------------------------
# ðŸ§  Example usage
# --------------------------------------------------------------------------
if __name__ == "__main__":
    service = QdrantService(
        url="http://34.87.152.106:6333",
        collection_name="toeic_speaking_knowledge"
    )

    # service.create_collection(vector_size=1024, recreate=False)

    # # Upsert example
    # service.upsert_points([
    #     {
    #         "id": 1,
    #         "vector": [0.1, 0.2, 0.3, 0.4] * 256,  # 1024 dims
    #         "payload": {"title": "Example", "content": "Hello Qdrant!"}
    #     }
    # ])

    # Search example
    result = service.search(question='what is hope?')
    print(result)
