"""
Standalone service for analyzing TOEIC test results with personalization support.
This service uses utilities from RouteNode but is independent from chat workflow.
"""

from typing import Dict, Any, Optional, List
from app.configs.model_config import Gemini
from app.services.qdrant_service import QdrantService
from app.db.session import get_db_session
import json
import re

try:
    from app.models.user_profile import get_personalization_context
    from app.services.db_profile_service import DatabaseProfileService
    PERSONALIZATION_AVAILABLE = True
except ImportError:
    PERSONALIZATION_AVAILABLE = False
    get_personalization_context = None
    DatabaseProfileService = None

# Optional sklearn imports for TF-IDF/MMR
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    TfidfVectorizer = None
    cosine_similarity = None
    np = None


class AnalyzeTestService:
    """
    Standalone service for analyzing TOEIC test results.
    Provides personalized feedback based on user profile and RAG-enhanced context.
    """
    
    def __init__(self):
        """Initialize the service with AI model and supporting services."""
        try:
            self._gemini = Gemini()
            self._llm = self._gemini.llm()
            self._vector_service = QdrantService()
            self._db_session = get_db_session()
            
            # Initialize profile service if available
            if PERSONALIZATION_AVAILABLE and self._db_session:
                self._db_profile_service = DatabaseProfileService(self._db_session)
            else:
                self._db_profile_service = None
                
        except Exception as e:
            raise Exception(f"Failed to initialize AnalyzeTestService: {str(e)}")
    
    def _refine_query_with_tfidf_mmr(self, query: str, corpus: List[str] = None, top_n=10, Œª=0.7) -> str:
        """
        Enhance query using TF-IDF and MMR for better RAG results.
        Reused from RouteNode implementation.
        """
        if not SKLEARN_AVAILABLE or not query.strip():
            return query
        
        try:
            # TOEIC-specific corpus
            if corpus is None:
                toeic_corpus = [
                    "TOEIC listening comprehension practice",
                    "TOEIC reading comprehension strategies", 
                    "TOEIC grammar rules and examples",
                    "TOEIC vocabulary building exercises",
                    "TOEIC test preparation tips",
                    "TOEIC common mistakes and patterns",
                    "TOEIC score improvement strategies"
                ]
                corpus = toeic_corpus

            vectorizer = TfidfVectorizer(
                ngram_range=(1, 2), 
                stop_words='english',
                max_features=1000,
                min_df=1,
                lowercase=True
            )
            
            all_texts = corpus + [query]
            tfidf = vectorizer.fit_transform(all_texts)
            feature_names = vectorizer.get_feature_names_out()
            query_vec = tfidf[-1].toarray()[0]

            # Get top terms
            top_indices = np.argsort(query_vec)[::-1][:top_n]
            top_terms = [feature_names[i] for i in top_indices if query_vec[i] > 0]

            if len(top_terms) < 2:
                return query

            # Apply MMR for diversity
            term_vecs = vectorizer.transform(top_terms).toarray()
            query_center = np.mean(term_vecs, axis=0)
            
            selected_ids = self._mmr_select(
                query_center, 
                term_vecs, 
                Œª=Œª, 
                top_k=min(5, len(term_vecs))
            )

            selected_terms = [top_terms[i] for i in selected_ids]
            refined_query = f"{query}. Related TOEIC concepts: {', '.join(selected_terms)}"
            
            print(f"üîç Query refinement: {query} -> {len(selected_terms)} terms added")
            return refined_query

        except Exception as e:
            print(f"‚ùå TF-IDF/MMR Error: {e}")
            return query
    
    def _mmr_select(self, query_vec, candidate_vecs, Œª=0.7, top_k=5):
        """Maximum Marginal Relevance selection for diversity."""
        if not SKLEARN_AVAILABLE:
            return list(range(min(top_k, len(candidate_vecs))))
        
        sim_query = cosine_similarity([query_vec], candidate_vecs)[0]
        sim_cand = cosine_similarity(candidate_vecs)

        selected = []
        candidates = list(range(len(candidate_vecs)))

        while len(selected) < top_k and candidates:
            scores = [
                Œª * sim_query[i] - (1 - Œª) * max([sim_cand[i][j] for j in selected] or [0])
                for i in candidates
            ]
            chosen = candidates[np.argmax(scores)]
            selected.append(chosen)
            candidates.remove(chosen)

        return selected
    
    def _get_user_profile(self, user_id: str):
        """Get user profile from database."""
        if not self._db_profile_service or not user_id:
            return None
        try:
            profile = self._db_profile_service.get_user_with_progress(user_id)
            return profile
        except Exception as e:
            print(f"Error getting user profile: {e}")
            return None
    
    def _search_context(self, query: str) -> str:
        """Search for relevant TOEIC context using RAG with enhancement."""
        try:
            # Refine query with TF-IDF/MMR
            refined_query = self._refine_query_with_tfidf_mmr(
                query=query,
                corpus=None,
                top_n=12,
                Œª=0.7
            )

            # Search Qdrant
            results = self._vector_service.search(
                question=refined_query,
                limit=5,
                score_threshold=0.3
            )
            
            # Extract content
            retrieved = [r["payload"].get("content", "") for r in results]
            context_text = "\n".join(f"- {txt}" for txt in retrieved if txt)
            
            return context_text or "No specific context found."
            
        except Exception as e:
            print(f"‚ùå RAG Error: {e}")
            return "No context available."
    
    def _create_analysis_prompt(self, test_data: dict, context: str, user_profile=None) -> str:
        """Create personalized analysis prompt."""
        
        # Base prompt
        base_prompt = """
B·∫°n l√† m·ªôt **chuy√™n gia TOEIC v√† tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh**. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† **ph√¢n t√≠ch k·∫øt qu·∫£ l√†m b√†i TOEIC c·ªßa ng∆∞·ªùi h·ªçc** b√™n d∆∞·ªõi v√† **tr·∫£ v·ªÅ ph·∫£n h·ªìi b·∫±ng JSON, vi·∫øt ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát**.

C·∫•u tr√∫c c·ªßa b√†i thi TOEIC g·ªìm hai ph·∫ßn (ƒëi·ªÉm t·ªëi ƒëa 990): Listening (ƒëi·ªÉm t·ªëi ƒëa 495) v√† Reading (ƒëi·ªÉm t·ªëi ƒëa 495).
"""
        
        # Add personalization if available
        if user_profile and PERSONALIZATION_AVAILABLE:
            personalization_ctx = get_personalization_context(user_profile)
            user_name = user_profile.display_name if hasattr(user_profile, 'display_name') else "b·∫°n"
            learning_style = personalization_ctx.get("learning_style", "balanced")
            skill_focus = personalization_ctx.get("skill_focus", [])
            
            base_prompt += f"""
**Th√¥ng tin ng∆∞·ªùi h·ªçc:**
- T√™n: {user_name}
- Phong c√°ch h·ªçc: {learning_style}
- K·ªπ nƒÉng c·∫ßn t·∫≠p trung: {', '.join(skill_focus) if skill_focus else 'Ch∆∞a x√°c ƒë·ªãnh'}

H√£y c√° nh√¢n h√≥a feedback d·ª±a tr√™n th√¥ng tin n√†y.
"""
        
        base_prompt += f"""
**Ng·ªØ c·∫£nh tham kh·∫£o t·ª´ c∆° s·ªü d·ªØ li·ªáu:**
{context}

Ph√¢n t√≠ch c·ªßa b·∫°n c·∫ßn:
- ƒê√°nh gi√° ƒëi·ªÉm m·∫°nh, ƒëi·ªÉm y·∫øu c·ªßa ng∆∞·ªùi h·ªçc theo t·ª´ng k·ªπ nƒÉng (Listening, Reading, v√† k·ªπ nƒÉng con).
- Ph√°t hi·ªán c√°c m·∫´u l·ªói th∆∞·ªùng g·∫∑p (v√≠ d·ª•: ng·ªØ ph√°p, t·ª´ v·ª±ng, suy lu·∫≠n, nghe nh·∫ßm t·ª´ kh√≥a...).
- Gi·∫£i th√≠ch ng·∫Øn g·ªçn nh∆∞ng s√¢u s·∫Øc, gi√∫p ng∆∞·ªùi h·ªçc hi·ªÉu nguy√™n nh√¢n sai.
- ƒê∆∞a ra c√°c g·ª£i √Ω h·ªçc t·∫≠p c·ª• th·ªÉ (nh∆∞ "luy·ªán Part 3 - h·ªôi tho·∫°i c√≥ 3 ng∆∞·ªùi n√≥i", ho·∫∑c "√¥n l·∫°i c·∫•u tr√∫c gi·ªõi t·ª´ ch·ªâ nguy√™n nh√¢n").

K·∫øt qu·∫£ tr·∫£ v·ªÅ ph·∫£i ƒë√∫ng c·∫•u tr√∫c sau:
{{
  "summary": {{
    "totalScore": number, // t·ªïng ƒëi·ªÉm (0-990)
    "listeningScore": number, // ƒëi·ªÉm Listening (0-495)
    "readingScore": number, // ƒëi·ªÉm Reading (0-495)
    "accuracy": string, // ƒë·ªô ch√≠nh x√°c chung (%)
    "comment": string // nh·∫≠n x√©t t·ªïng quan v·ªÅ b√†i thi
  }},
  "weakSkills": [string], // li·ªát k√™ k·ªπ nƒÉng y·∫øu
  "mistakePatterns": [string], // m√¥ t·∫£ d·∫°ng l·ªói ph·ªï bi·∫øn
  "recommendations": [string] // g·ª£i √Ω h·ªçc t·∫≠p c·ª• th·ªÉ, r√µ r√†ng
}}

H√£y vi·∫øt ph·∫£n h·ªìi b·∫±ng **ti·∫øng Vi·ªát t·ª± nhi√™n, th√¢n thi·ªán, kh√≠ch l·ªá ng∆∞·ªùi h·ªçc**, kh√¥ng d√πng thu·∫≠t ng·ªØ qu√° h√†n l√¢m.

**D·ªØ li·ªáu c·∫ßn ph√¢n t√≠ch:**
{json.dumps(test_data, ensure_ascii=False, indent=2)}
"""
        
        return base_prompt
    
    def _parse_ai_response(self, response_text: str) -> dict:
        """Parse and clean AI response."""
        try:
            if not response_text or not response_text.strip():
                raise Exception("AI response is empty")
            
            # Log the raw response for debugging
            print(f"üìù Raw AI response (first 500 chars): {response_text[:500]}")
            
            # Try to extract JSON from markdown code block first
            json_match = re.search(r'```json\s*\n(.*?)\n```', response_text, re.DOTALL)
            if json_match:
                cleaned = json_match.group(1).strip()
                print(f"‚úÖ Extracted JSON from markdown code block")
            else:
                # Fallback: remove markdown code blocks if present
                cleaned = re.sub(r"^```json\n|```$", "", response_text.strip(), flags=re.MULTILINE)
                print(f"‚ö†Ô∏è No markdown code block found, using fallback cleaning")
            
            # Try to parse JSON
            result = json.loads(cleaned)
            print(f"‚úÖ Successfully parsed JSON")
            return result
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Parse Error: {str(e)}")
            print(f"üìÑ Failed response text: {response_text[:1000]}")
            raise Exception(f"Failed to parse AI response as JSON: {str(e)}")
        except Exception as e:
            print(f"‚ùå Unexpected error in parsing: {str(e)}")
            raise Exception(f"Failed to parse AI response: {str(e)}")
    
    async def analyze_test_result(
        self, 
        test_data: dict, 
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze TOEIC test results with optional personalization.
        
        Args:
            test_data: Test results including scores, answers, questions
            user_id: Optional user ID for personalization
            
        Returns:
            Structured analysis result with summary, weak skills, patterns, and recommendations
        """
        try:
            # Get user profile if available
            user_profile = None
            if user_id:
                user_profile = self._get_user_profile(user_id)
            
            # Search for relevant context
            search_query = f"TOEIC test analysis common mistakes patterns score {test_data.get('totalScore', 0)}"
            context = self._search_context(search_query)
            
            # Create personalized prompt
            prompt = self._create_analysis_prompt(test_data, context, user_profile)
            
            # Call Gemini for analysis
            from langchain.schema.messages import HumanMessage
            result = self._llm.invoke([HumanMessage(content=prompt)])
            
            # Parse response
            analysis = self._parse_ai_response(result.content)
            
            # Add metadata
            analysis["metadata"] = {
                "personalized": user_profile is not None,
                "user_id": user_id,
                "context_used": bool(context and context != "No context available.")
            }
            
            return analysis
            
        except Exception as e:
            print(f"‚ùå Analysis failed: {e}")
            raise Exception(f"Failed to analyze test result: {str(e)}")
